{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome","text":"<p>          A         Kubernetes          controller for Prometheus Anomaly Detection      </p> <p> </p>"},{"location":"#k8s-anomaly-detection-operator","title":"K8S Anomaly Detection Operator","text":"<p>Visit the GitHub repository at https://github.com/amitde69/anomaly-operator to see examples, raise issues, and to contribute to the project.</p>"},{"location":"#what-is-it","title":"What is it?","text":"<p>K8S Anomaly Detection Operator is a controller to help manage Detector deployments for a Kubernetes cluster using CRDs (Custom Resource Defenitions). Detector deployments allow you to configure Prometheus endpoints + PromQL expressions and some extra configuration to monitor/alert on anomalies found for a given timeframe.</p>"},{"location":"#why-would-i-use-it","title":"Why would I use it?","text":"<p>This operator lets you choose metrics and fine tune the configuration in order to detect anomalies over time inside of your system in a simple, scalable manner, preempting events such as regular, repeated high load. This allows for proactive rather than simply reactive maintenance of production environments and make intelligent ahead of time decisions.</p>"},{"location":"#what-systems-would-need-it","title":"What systems would need it?","text":"<p>Systems that have predictable trends in metrics, for example; if over a 24 hour period the load on a resource is generally higher between 3pm and 5pm - with enough data and use of correct configurations the detector could expose an anomaly and push a notification to the relevant team in order to look deeper into it, increasing responsiveness of the system to changes in metrics.</p>"},{"location":"#features","title":"Features","text":"<ul> <li>Integrates simply with Prometheus metrics.</li> <li>Leverages Prophet framework by facebook https://github.com/facebook/prophet</li> <li>Allows customization of Kubernetes resource spec. Can work on managed solutions such as EKS or GKE.</li> <li>Light weight and scalable.</li> <li>Simple configuration and easy integration.</li> </ul>"},{"location":"architecture/","title":"Detector Deployment and Architecture","text":"<p>K8S Anomaly Detector Operator supports creating a CR which propagtes a few other Kubernetes resources:</p> <ul> <li><code>Deployment</code></li> <li><code>ConfigMap</code></li> <li><code>ServiceAccount</code></li> </ul> <p>The name of the Detector CR is propagated to the <code>labels</code> <code>selectors</code> and <code>name</code> of every resource it creates. <pre><code>...\nkind: Detector\nmetadata:\nname: detector-name\n...\n</code></pre></p> <pre><code>&gt; kubectl get cm,deploy,sa -n detectors\n\nNAME                              DATA   AGE\nconfigmap/detector-name           1      5m\n\nNAME                              READY   AGE\ndeployment.apps/detector-name     1/1     5m\n\nNAME                              SECRETS   AGE\nserviceaccount/detector-name      0         5m\n</code></pre>"},{"location":"architecture/#deployment","title":"Deployment","text":"<p>The <code>Deployment</code> resource is desgined to run as a single replica pod.</p> <p>Tip</p> <p>If a Detector <code>queries</code> list is too long or the queries are too heavy it is reccomended to split it to smaller and distributed Detector resources</p>"},{"location":"architecture/#configmap","title":"ConfigMap","text":"<p>The <code>ConfigMap</code> resource is created with the detector configurations from the Detector resource.</p> <p>Note</p> <p>If the spec of the Detector changes the <code>ConfigMap</code> will be updated with the new config  and the <code>Deployment</code> will be rolled out in order to load the new <code>ConfigMap</code>.</p>"},{"location":"architecture/#serviceaccount","title":"ServiceAccount","text":"<p>The <code>ServiceAccount</code> resource is created by default and cannot be changed or modified. It is used as the <code>Deployment</code> serviceaccount and can be changed to a custom serviceaccount using the <code>pod_spec</code>  on the Detector resource.</p>"},{"location":"code-of-conduct/","title":"Kubernetes Community Code of Conduct","text":"<p>Please refer to our Kubernetes Community Code of Conduct</p>"},{"location":"getting_started/examples/","title":"Detector configuration options and examples","text":"<p>This document covers configuration of a <code>Detector</code> resource.</p> <p>When configuring a <code>Detector</code> resource there are a few required fields and optional fields</p>"},{"location":"getting_started/examples/#minimal-spec","title":"Minimal Spec","text":"<p>An example minimal spec with only required fields would be as follows:</p> <pre><code>apiVersion: monitoring.amitdebachar/v1alpha1\nkind: Detector\nmetadata:\nname: detector-name\nspec:\n\n## (required) The detector image\nimage: \"amitde7896/anomaly-operator:latest-detector\"\n\n## (required) Promethus HTTP API endpoint \nprom_url: \"http://prometheus.monitoring.svc.cluster.local\"\n\n## (required) Evaluation interval in minutes in which the Detector\n## will query Prometheus and analyze for anomalies\n## Note: use float for under 1 min interval e.g. \"0.01\"  \ninterval_mins: \"15\"\n\n## (required) List of PromQL expressions which the detector \n## will query and evaluate using the configured Prometheus endpoint and interval   \nqueries:\n\n## (required) Query name which will appear in the logs and metrics \n## in order to correlate the detected anomaly to the configured query  \n- name: \"sum_pods_running_anomaly\"\n\n## (required) Query PromQL expression\nquery: 'sum(kube_pod_status_phase{phase=~\"Running\", pod=~\"application-pod-.*\"}) &gt; 1'\n\n## (required) parse_datetime formated text such as - \"1m\" \"4h\" \"2d\" \"3w\"\n## Past period for which the detector will train and learn the trend\n## Note: bigger value means longer query time\ntrain_window: \"14d\"\n</code></pre>"},{"location":"getting_started/examples/#custom-anomaly-spec","title":"Custom Anomaly Spec","text":"<p>You can tune some parameters in order to avoid misclassifying an anomaly.</p> <p>An example custom anomaly spec with optional fields would be as follows:</p> <pre><code>apiVersion: monitoring.amitdebachar/v1alpha1\nkind: Detector\nmetadata:\nname: detector-name\nspec:\n...\nqueries:\n- name: \"sum_pods_running_anomaly\"\nquery: 'sum(kube_pod_status_phase{phase=~\"Running\", pod=~\"application-pod-.*\"}) &gt; 1'\ntrain_window: \"14d\"\n\n## (optional) parse_datetime formated text such as - \"1m\" \"4h\" \"2d\" \"3w\"\n## Past period for which the detector will evaluate anomalies based on \"train_window\" period\n## Default: \"1h\" \ndetection_window_hours: \"2h\"\n\n## (optional) float value which tunes the Prophet parameter \"changepoint_prior_scale\"\n## https://facebook.github.io/prophet/docs/trend_changepoints.html#adjusting-trend-flexibility\n## This will affect how flexible or stiff the model is between change points (date points)\n## Note: Lower is stiffer Higher is more flexible  \n## Default: \"0.05\" \nflexibility: \"10\"\n\n## (optional) integer value which is used for PromQL request in the \"step\" paramter\n## This will affect how many data points the detector is getting from Prometheus for a given timeframe\n## Note: Higher means longer query time and more sensitive to anomalies\n## Default: number of hours in \"train_window\"\n## Disclaimer: setting too small value for a longer \"train_window\" can cause PromQL to fail\nresolution: 1400\n\n## (optional) integer value represent precetange \n## which is used to increase/decrease the buffer the detector will append to the detection threshold \n## Note: Higher means less likely to find anomalies\n## Default: 100\nbuffer_pct: 150\n</code></pre>"},{"location":"getting_started/examples/#custom-pod-spec","title":"Custom Pod Spec","text":"<p>You can override the pod template and spec used for the detector pod.</p> <p>Warning</p> <p>The override should be mindful of the default pod spec that is created by the operator.</p> <p>It is reccommended to at least use the same default spec and add more options and not change the existing ones such as <code>env</code> section and <code>volumeMounts</code>/<code>volumes</code>.</p> <p>Change at your own risk. </p> <p>Warning</p> <p>Both <code>image</code> and the <code>pod_spec.spec.containers.*.image</code> are required in this case but only the <code>containers</code> section will determine the image used in the created deployment</p> <p>An example custom pod spec would be as follows:</p> <pre><code>apiVersion: monitoring.amitdebachar/v1alpha1\nkind: Detector\nmetadata:\nname: detector-name\nspec:\nimage: \"...\"\n...\npod_spec:\nspec:\ncontainers:\n- env:\n- name: LOG_LEVEL\nvalue: INFO\nimage: amitde7896/anomaly-operator:latest-detector\nimagePullPolicy: Always\nname: custom-detector-spec\nports:\n- containerPort: 9090\nname: http\nprotocol: TCP\nresources:\nlimits:\ncpu: 1500m\nmemory: 1G\nrequests:\ncpu: 1500m\nmemory: 1G\nvolumeMounts:\n- mountPath: /app/config.yaml\nname: detector-name\nsubPath: detector-name-conf.yaml\nserviceAccount: detector-name\nserviceAccountName: detector-name\nvolumes:\n- configMap:\ndefaultMode: 420\nname: detector-name\nname: detector-name\n...\nqueries:\n...\n</code></pre>"},{"location":"getting_started/logs/","title":"Logs","text":"<p>In order to investigate an anomaly that was detected, the detector pod will output logs to STDOUT with the relevant information.</p>"},{"location":"getting_started/logs/#example","title":"Example","text":"<p>For the following <code>Detector</code>: <pre><code>apiVersion: monitoring.amitdebachar/v1alpha1\nkind: Detector\nmetadata:\nname: minimal-detector\nspec:\nimage: \"amitde7896/anomaly-operator:latest-detector\"\nprom_url: \"http://prometheus.monitoring.svc.cluster.local\"\ninterval_mins: \"15\"\nqueries:\n- name: \"sum_pods_running_anomaly\"\nquery: 'sum(kube_pod_labels{label_app=~\"example-application-.*\"}) by (label_app) &gt; 1'\ntrain_window: \"14d\"\n</code></pre> The following logs will be printed when anomaly detected: <pre><code>{\n\"timestamp\": \"2023-04-12T22:03:05.261Z\",\n\"level\": \"WARNING\",\n\"message\": \"Found 1 anomalies for sum_pods_running_anomaly in {'label_app': 'example-application-b'}\"\n}\n{\n\"timestamp\": \"2023-04-12T22:03:05.378Z\",\n\"level\": \"WARNING\",\n\"message\": \"[sum_pods_running_anomaly] {'label_app': 'example-application-b'} time: 2023-04-12 21:41:51 value: 12\"\n}\n</code></pre></p> <p>Note</p> <p>In cases where a single query returns many different results, the log will contain the returned values from the Prometheus response. For example in the above <code>Detector</code> a <code>by (label_app)</code> was added to the PromQL therefore the <code>label_app</code> value was added to the log for the detected anomaly.</p>"},{"location":"getting_started/metrics/","title":"Metrics","text":"<p>The detector pod exposes Prometheus metrics at <code>/metrics</code> on port 9090 by default.</p> <p>Note</p> <p>You will have to create your own <code>Service</code> and <code>ScrapeConfig</code> or <code>ServiceMonitor</code> to expose and scrape the metrics. See example</p> <p><code>anomaly_counter</code> - Total number of anomalies found for the last interval. </p> <p>You can create your own rules/alerts to get notified whenever an anomaly was detected based on the metrics.</p>"},{"location":"getting_started/operator_deployment/","title":"K8S Anomaly Detection Operator Deployment","text":""},{"location":"getting_started/operator_deployment/#deploy-helm-chart","title":"Deploy Helm Chart","text":"<p>The operator helm chart can be simply cloned and deployed from this directory  https://github.com/amitde69/anomaly-operator/tree/main/helm."},{"location":"getting_started/operator_deployment/#deploy-operator-to-cluster","title":"Deploy Operator to Cluster","text":"<p>We recommend using the Helm chart.</p> Via HelmVia YAML manifests <ol> <li>Clone the Operator repo <pre><code>git clone https://github.com/amitde69/anomaly-operator\n</code></pre></li> <li> <p>Install the chart via <code>helm install</code>. <pre><code>helm install anomaly-operator helm/\n</code></pre></p> <p>Note</p> <p>The <code>helm install</code> command automatically applies the CRDs.</p> </li> </ol> <p>Helm install command to override image repo and tag :  <pre><code>helm install anomaly-operator helm \\\n    --set repo=\"xxx/anomaly-operator\" \\\n    --set tag=\"xxx-operator\"\n</code></pre></p> <p>Helm install command to override namespace :  <pre><code>helm install anomaly-operator helm -n custom-namespace\n</code></pre></p> <ol> <li>Clone the Operator repo <pre><code>git clone https://github.com/amitde69/anomaly-operator\n</code></pre></li> <li>Template The Helm Chart <pre><code>helm template helm &gt; deploy.yaml\n</code></pre></li> <li>Apply the deploy.yaml <pre><code>kubectl apply -f deploy.yaml\n</code></pre></li> </ol> <p>Helm template command to override image repo and tag :  <pre><code>helm template helm \\\n    --set repo=\"xxx/anomaly-operator\" \\\n    --set tag=\"xxx-operator\" &gt; deploy.yaml\n</code></pre></p> <p>Helm template command to override namespace :  <pre><code>helm install helm -n custom-namespace &gt; deploy.yaml\n</code></pre></p>"},{"location":"getting_started/operator_deployment/#upgrade-the-operator","title":"Upgrade The Operator","text":"<p>The operator doesn't receive security updates automatically. You need to manually upgrade to a newer version when it becomes available.</p>"}]}